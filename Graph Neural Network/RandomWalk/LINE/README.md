# LINE : Large-scale information network embedding

Category: Graph Neural Network<br>
Year: 2019<br>
작성일: May 21, 2021

# Contributions
---
![Untitled](https://user-images.githubusercontent.com/28617444/126431884-fd4a997d-460e-4153-9bb7-4acedc0fbe80.png)


### 1. a novel network embedding model LINE

- 복잡한 구조의 network에 효과적
- **first-order와 second-order proximities를 모두 다루는 object function 사용**
    - 기존 DeepWalk 모델 : nodes with higher second-order proximity

### 2. edge-sampling algorithm for optimizing the objective

- 기존의 SGD 알고리즘은 매우 많은 edge 가중치가 존재하므로, multiply 시 gradient explosion 문제가 발생
- 따라서, 본 논문에서는 효율적인 negative sampling 제안
    - weight를 update하는데에 있어서 모든 edge를 대상으로 하지 않고, 일부를 sample하는 방식

### 3. conduct extensive experiments on real-word information networks

- 실험 결과가 LINE model의 효율성 입증

# Problem Definition
---

### Definition 1. Information Network

G = (V, E)

- V : set of vertices
- E : set of edges between the vertices
    - edge의 weights는 binary나 real value를 가질 수 있다.
- 해당 논문에서는 only non-negative edge weights 만 고려

    EX) $w_{uv}$ : binary values

    - 다른 object 간의 co-occurrence network이므로, any non-negative value를 가진다.
    - edge의 weights는 동시에 많이 co-occur할 수도, 또는 몇번만 co-occur 할 수도 있기에 diverge한다. (발산한다?)
- vertices 간의 local pairwise proximity와 같은 local network 구조는 preserved되어야 한다.
    - 논문에서는 이러한 local network structure를 vertices 간의 *first-order proximity*라 한다.

### Definition 2. (First-Order Proximity)

- = Local pairwise proximity
- real world network 에서 two node 의 유사성을 나타낸다.
    - 그러나, 실제로 link는 매우 적은 비율로 존재하고, 대부분 missing일 것
    - 따라서, first-order proximity를 혼자 사용하는 것은 네트워크 구조를 preserve하는데 부족하다.
    - sparsity 문제를 다루는 대안이 필요함
        - Definition 3

### Definition 3. (Second-Order Proximity)

- vertex u와 v 간의 second-order proximity : $p_u$와 $p_v$ 간의 유사도로 결정됨

### Definition 4. (Large-scale Information Network Embedding)

- represent each vertex v ∈ V into a low-dimensional space $R^d$
    - space $R^d$ 에서는 vertices 간의 first, second order proximity가 반영될 것이다!

# LINE: LARGE-SCALE INFORMATION
NETWORK EMBEDDING

---

## 1 Model Description

### 1.1 Line with First-order Proximity

- undirected edge (i,j) 에서 edge의 양 끝에 연결된 두 개의 vertex는 v_i와 v_j
- 이 둘의 **joint probability**(결합확률)
![Untitled 1](https://user-images.githubusercontent.com/28617444/126431885-76c6cbd2-a710-4c4d-b6e7-0511f8244043.png)


    - 두 vertex 가 유사 ⇒ 내적이 커짐 ⇒ 분모가 작아짐 ⇒ P가 커짐
    - 두 vertex가 유사X ⇒ 내적이 작음 ⇒ 분모가 커짐 ⇒ P가 작아짐


- **empirical probability**
![Untitled 2](https://user-images.githubusercontent.com/28617444/126431887-eba6f38c-49cf-4d28-aa13-65c0bfadb7b9.png)

    - 특정 edge의 weight 모든 edge의 weight의 합 : 해당 edge의 비중

- objective function

    ![Untitled 3](https://user-images.githubusercontent.com/28617444/126431888-2185da6c-4f01-48dd-b5b2-47c806b9aa28.png)


  - 거리 d가 KL-divergence로 표현

    ![Untitled 4](https://user-images.githubusercontent.com/28617444/126431889-d9a5ac7b-ae87-4af9-873f-424055f26e44.png)

    - 두 분포의 KL-Divergence를 최소화

- first-order proximity를 반영하는 network embedding model: minimize하는 방향으로 optimize
- 그러나 only applicable for undirected graph, not for directed graph

### 1.2 Line with Second-order Proximity

- directed graph , undirected graph에서도 모두 적용 가능
- vertex 의 2가지 역할
    1. "vertex itself" : u_i
    2. "specific context of other vertices" : u_i'


- probability of "context" v_j generated by vertext v_i
![Untitled 5](https://user-images.githubusercontent.com/28617444/126431890-52967a61-8b85-46de-8541-8b904657ccfd.png)

  - |V| : number of vertices of "contexts"
  - 확장

      ![Untitled 6](https://user-images.githubusercontent.com/28617444/126431877-bd9b4ae9-65aa-48cc-9393-4bc880d0d608.png)

- objective function

![Untitled 7](https://user-images.githubusercontent.com/28617444/126431879-9375477a-4991-45ed-951a-10a712f451ca.png)


  - d : 두 분포의 거리
  - $\lambda$ : network에서 vertex별로 다른 중요도 ⇒ $d_i$로 표기

  - KL 다이버전스 ( omitting some Constrants $d_i$
    ![Untitled 8](https://user-images.githubusercontent.com/28617444/126431880-9f237f31-3f68-477a-b1b6-b0e094e0797a.png)


### 1.3 Combining first-order and second-order proximities

- 두 가지 proximity를 이용한 objective function으로 각각 따로 훈련 후, embedding된 것을 concatenate하여 표현

## 2. Model Optimization

- 목적식이 계산복잡도가 높기 때문에, negative sampling approach를 사용한다.

![Untitled 9](https://user-images.githubusercontent.com/28617444/126431881-262f141a-a1e9-4159-a807-3fad6cba99e3.png)


  - 시그모이드 fucntion
  - first term model : the observed edges
  - second term model : negative edges from the noise distribution
  - k : negative edge의 수

- ASGD(asynchronous stochastic gradient algorithm) 사용
![Untitled 10](https://user-images.githubusercontent.com/28617444/126431883-e8581303-a194-4056-bcca-8cbd20ee02d1.png)
